{"provider":"autogen_agentchat.agents.AssistantAgent","component_type":"agent","version":1,"component_version":1,"description":"An agent that provides assistance with tool use.","label":"AssistantAgent","config":{"name":"AnalystAssistant","model_client":{"provider":"autogen_ext.models.openai.OpenAIChatCompletionClient","component_type":"model","version":1,"component_version":1,"description":"Chat completion client for OpenAI hosted models.","label":"OpenAIChatCompletionClient","config":{"model":"gpt-4o","api_key":"**********"}},"tools":[{"provider":"autogen_core.tools.FunctionTool","component_type":"tool","version":1,"component_version":1,"description":"Create custom tools by wrapping standard Python functions.","label":"FunctionTool","config":{"source_code":"async def load_csv_file(csv_url: str) -> str:\n    \"\"\"\n    Load and validate a CSV file containing alert data.\n\n    Args:\n        csv_url: URL or path to the CSV file\n\n    Returns:\n        Status message\n    \"\"\"\n    try:\n        # Just check if we can load the file\n        import requests\n        if csv_url.startswith('http'):\n            response = requests.head(csv_url)\n            if response.status_code != 200:\n                return f\"Error: Could not access file at {csv_url}\"\n        else:\n            if not os.path.exists(csv_url):\n                return f\"Error: File not found at {csv_url}\"\n\n        return f\"CSV file validated and ready for processing: {csv_url}\"\n    except Exception as e:\n        return f\"Error validating CSV file: {str(e)}\"\n","name":"load_csv_file","description":"\n    Load and validate a CSV file containing alert data.\n    \n    Args:\n        csv_url: URL or path to the CSV file\n        \n    Returns:\n        Status message\n    ","global_imports":[],"has_cancellation_support":false}},{"provider":"autogen_core.tools.FunctionTool","component_type":"tool","version":1,"component_version":1,"description":"Create custom tools by wrapping standard Python functions.","label":"FunctionTool","config":{"source_code":"async def run_alert_processing(csv_url: str) -> str:\n    \"\"\"\n    Process alerts from the CSV file.\n\n    Args:\n        csv_url: URL or path to the CSV file\n\n    Returns:\n        Processing results summary\n    \"\"\"\n    from playwright.async_api import async_playwright\n    from io import BytesIO\n    import requests\n    import datetime\n    import logging\n    import pandas as pd\n    from typing import Tuple, Dict, List\n    from supabase import create_client, Client\n    import json\n    import os\n\n    class SupabaseStorage:\n        def __init__(self):\n            self.supabase_url = os.environ.get(\"SUPABASE_URL\")\n            self.supabase_key = os.environ.get(\"SUPABASE_KEY\")\n            self.client = create_client(self.supabase_url, self.supabase_key)\n            self.bucket_name = \"ubs\"\n\n            # Create bucket if it doesn't exist\n            try:\n                self.client.storage.get_bucket(self.bucket_name)\n            except Exception as e:\n                logger.info(f\"Creating bucket {self.bucket_name}: {str(e)}\")\n                self.client.storage.create_bucket(self.bucket_name)\n\n        def upload_file(self, file_path, file_name=None):\n            \"\"\"Upload a file to Supabase Storage\"\"\"\n            if file_name is None:\n                file_name = os.path.basename(file_path)\n\n            with open(file_path, \"rb\") as f:\n                file_data = f.read()\n\n            return self.upload_binary(file_data, file_name)\n\n        def upload_binary(self, binary_data, file_name):\n            \"\"\"Upload binary data to Supabase Storage\"\"\"\n            # Generate a unique filename to avoid collisions\n            unique_filename = f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{file_name}\"\n\n            # Upload to Supabase Storage\n            self.client.storage.from_(self.bucket_name).upload(\n                path=unique_filename,\n                file=binary_data,\n                file_options={\"content-type\": \"application/octet-stream\"}\n            )\n\n            # Get public URL\n            file_url = self.client.storage.from_(self.bucket_name).get_public_url(unique_filename)\n            return file_url\n\n        def save_json_data(self, data, file_name):\n            \"\"\"Save JSON data to Supabase Storage\"\"\"\n            json_str = json.dumps(data, indent=2)\n            return self.upload_binary(json_str.encode('utf-8'), file_name)\n\n    class MarketTypeValidator:\n        \"\"\"Validates if a security is traded on a regulated market or growth market\"\"\"\n\n        def __init__(self, storage_manager):\n            self.storage_manager = storage_manager\n            # Create temp directory for screenshots if it doesn't exist\n            os.makedirs(\"temp\", exist_ok=True)\n\n        async def check_market_type(self, isin: str) -> Tuple[bool, str, str, str]:\n            \"\"\"\n            Check the market type for a given ISIN\n\n            Args:\n                isin: The ISIN to check\n\n            Returns:\n                Tuple containing:\n                - is_regulated: True if traded on a regulated market\n                - market_type: The identified market type\n                - source_url: The URL that was used to get this information\n                - evidence_url: URL to the screenshot evidence in Supabase\n            \"\"\"\n            # First, determine the country from the ISIN\n            country_code = isin[:2]\n\n            if country_code == \"DE\":\n                return await self._check_german_market(isin)\n            elif country_code == \"FR\":\n                return await self._check_french_market(isin)\n            else:\n                # For other countries, we'd add similar methods\n                logger.warning(f\"No specific market validation implemented for country {country_code}\")\n                return None, f\"Unknown market for country {country_code}\", None, None\n\n        async def _check_german_market(self, isin: str) -> Tuple[bool, str, str, str]:\n            \"\"\"\n            Check if a German security is on a regulated market using boerse-frankfurt.de\n            \"\"\"\n            url = f\"https://www.boerse-frankfurt.de/aktie/{isin}\"\n            temp_screenshot_path = f\"temp/evidence_{isin}_german_market_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n\n            async with async_playwright() as p:\n                browser = await p.chromium.launch()\n                page = await browser.new_page()\n                evidence_url = None\n\n                try:\n                    await page.goto(url)\n                    await page.wait_for_selector(\".widget-table\", timeout=10000)\n\n                    # Scroll to the bottom of the page to ensure all content is loaded\n                    await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n\n                    # Allow some time for any lazy-loaded content to appear\n                    await asyncio.sleep(1)\n\n                    # Capture screenshot for evidence\n                    await page.screenshot(path=temp_screenshot_path, full_page=True)\n\n                    # Upload screenshot to Supabase\n                    evidence_url = self.storage_manager.upload_file(temp_screenshot_path)\n\n                    # Extract all table rows\n                    rows = await page.query_selector_all(\"table.widget-table tr\")\n\n                    market_type = \"Unknown\"\n                    for row in rows:\n                        cells = await row.query_selector_all(\"td\")\n                        if len(cells) == 2:\n                            key = (await cells[0].inner_text()).strip().lower()\n                            value = (await cells[1].inner_text()).strip().lower()\n\n                            if key == \"markt\":\n                                market_type = value\n                                break\n\n                    is_regulated = \"regulierter markt\" in market_type\n                    pretty_market = \"Regulated Market\" if is_regulated else \"Unregulated Market\" if market_type else \"Unknown Market\"\n                    page_url = page.url\n\n                    # Clean up the temp file\n                    if os.path.exists(temp_screenshot_path):\n                        os.remove(temp_screenshot_path)\n\n                    return is_regulated, pretty_market, page_url, evidence_url\n\n                except Exception as e:\n                    logger.error(f\"Error checking German market for {isin}: {e}\")\n                    return None, f\"Error checking market: {str(e)}\", url, evidence_url\n\n                finally:\n                    await browser.close()\n\n        async def _check_french_market(self, isin: str) -> Tuple[bool, str, str, str]:\n            \"\"\"Check if a French security is on a regulated market (via Euronext Paris).\"\"\"\n            url = f\"https://live.euronext.com/en/product/equities/{isin}-XPAR/market-information\"\n            temp_screenshot_path = f\"temp/evidence_{isin}_french_market_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n\n            async with async_playwright() as p:\n                browser = await p.chromium.launch()\n                page = await browser.new_page()\n                evidence_url = None\n\n                try:\n                    await page.goto(url)\n                    await page.wait_for_selector(\"div#fs_info_block table\", timeout=15000)\n\n                    # Scroll to the bottom of the page to ensure all content is loaded\n                    await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n\n                    # Allow some time for any lazy-loaded content to appear\n                    await asyncio.sleep(1)\n\n                    # Capture screenshot for evidence\n                    await page.screenshot(path=temp_screenshot_path, full_page=True)\n\n                    # Upload screenshot to Supabase\n                    evidence_url = self.storage_manager.upload_file(temp_screenshot_path)\n\n                    # Get all rows in the General Information table\n                    rows = await page.query_selector_all(\"div#fs_info_block table tr\")\n                    for row in rows:\n                        cells = await row.query_selector_all(\"td\")\n                        if len(cells) >= 2:\n                            key = (await cells[0].inner_text()).strip()\n                            value = (await cells[1].inner_text()).strip()\n                            if key == \"Market\":\n                                is_regulated = value.strip().lower() == \"euronext paris\"\n                                market_type = \"Regulated Market\" if is_regulated else \"Unregulated Market\"\n\n                                # Clean up the temp file\n                                if os.path.exists(temp_screenshot_path):\n                                    os.remove(temp_screenshot_path)\n\n                                return is_regulated, market_type, url, evidence_url\n\n                    # Clean up the temp file\n                    if os.path.exists(temp_screenshot_path):\n                        os.remove(temp_screenshot_path)\n\n                    return None, \"Market info not found\", url, evidence_url\n\n                except Exception as e:\n                    logger.error(f\"Error checking French market for {isin}: {e}\")\n                    # Clean up the temp file if it exists\n                    if os.path.exists(temp_screenshot_path):\n                        os.remove(temp_screenshot_path)\n                    return None, f\"Error checking market: {str(e)}\", url, evidence_url\n\n                finally:\n                    await browser.close()\n\n    class AlertProcessingSystem:\n        def __init__(self):\n            self.results = []\n            self.storage_manager = SupabaseStorage()\n\n        def load_csv_data(self, csv_url):\n            \"\"\"Load CSV data from URL\"\"\"\n            try:\n                # Check if it's a URL or local file\n                if csv_url.startswith('http'):\n                    # Download the file from URL\n                    response = requests.get(csv_url)\n                    if response.status_code == 200:\n                        # Check if Excel or CSV\n                        if csv_url.endswith('.xlsx') or csv_url.endswith('.xls'):\n                            return pd.read_excel(BytesIO(response.content))\n                        else:\n                            return pd.read_csv(BytesIO(response.content))\n                    else:\n                        raise Exception(f\"Failed to download file: {response.status_code}\")\n                else:\n                    # Check if Excel or CSV\n                    if csv_url.endswith('.xlsx') or csv_url.endswith('.xls'):\n                        return pd.read_excel(csv_url)\n                    else:\n                        return pd.read_csv(csv_url)\n            except Exception as e:\n                logger.error(f\"Error loading CSV data: {e}\")\n                raise\n\n        async def verify_market_type(self, isin, alert_id):\n            \"\"\"Verify market type for a security\"\"\"\n            validator = MarketTypeValidator(self.storage_manager)\n            is_regulated, market_type, source_url, evidence_url = await validator.check_market_type(isin)\n\n            # Return verification results\n            return {\n                \"alert_id\": alert_id,\n                \"isin\": isin,\n                \"is_regulated\": is_regulated,\n                \"market_type\": market_type,\n                \"source_url\": source_url,\n                \"evidence_url\": evidence_url,\n                \"verification_timestamp\": datetime.datetime.now().isoformat()\n            }\n\n        def dummy_verify_outstanding_shares(self, isin, company_name):\n            \"\"\"Dummy function for outstanding shares verification\"\"\"\n            return {\n                \"isin\": isin,\n                \"company_name\": company_name,\n                \"shares_verified\": True,\n                \"verification_timestamp\": datetime.datetime.now().isoformat(),\n                \"notes\": \"Dummy verification - this step was skipped as requested\"\n            }\n\n        def make_final_decision(self, market_verification_result):\n            \"\"\"Make final decision based on verification results\"\"\"\n            is_regulated = market_verification_result.get(\"is_regulated\")\n\n            if is_regulated is None:\n                decision = \"Inconclusive\"\n                justification = \"Could not determine market type\"\n            elif is_regulated:\n                decision = \"True Positive\"\n                justification = f\"Security is traded on a regulated market: {market_verification_result.get('market_type')}\"\n            else:\n                decision = \"False Positive\"\n                justification = f\"Security is traded on an unregulated market: {market_verification_result.get('market_type')}\"\n\n            return {\n                \"decision\": decision,\n                \"justification\": justification,\n                \"timestamp\": datetime.datetime.now().isoformat()\n            }\n\n        def document_alert_processing(self, alert_id, isin, company_name, market_verification, decision):\n            \"\"\"Document alert processing results\"\"\"\n            documentation = {\n                \"alert_id\": alert_id,\n                \"isin\": isin,\n                \"company_name\": company_name,\n                \"market_verification\": market_verification,\n                \"decision\": decision,\n                \"documentation_timestamp\": datetime.datetime.now().isoformat()\n            }\n\n            self.results.append(documentation)\n\n            # Create a text report and save to Supabase\n            report_filename = f\"report_{alert_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n            report_url = self.storage_manager.save_json_data(documentation, report_filename)\n\n            logger.info(f\"Report saved to Supabase: {report_url}\")\n\n            return documentation, report_url\n\n        def export_results(self):\n            \"\"\"Export processing results to Supabase in a human-readable format\"\"\"\n            if self.results:\n                # Create a more human-readable format with the most relevant fields\n                readable_results = []\n                for result in self.results:\n                    market_verification = result.get(\"market_verification\", {})\n                    decision = result.get(\"decision\", {})\n\n                    readable_result = {\n                        \"Alert ID\": result.get(\"alert_id\"),\n                        \"ISIN\": result.get(\"isin\"),\n                        \"Company Name\": result.get(\"company_name\"),\n                        \"Market Type\": market_verification.get(\"market_type\", \"Unknown\"),\n                        \"Is Regulated Market\": \"Yes\" if market_verification.get(\"is_regulated\") else \"No\" if market_verification.get(\"is_regulated\") is not None else \"Unknown\",\n                        \"Decision\": decision.get(\"decision\", \"Unknown\"),\n                        \"Justification\": decision.get(\"justification\", \"\"),\n                        \"Evidence URL\": market_verification.get(\"evidence_url\", \"\"),\n                        \"Source URL\": market_verification.get(\"source_url\", \"\"),\n                        \"Timestamp\": decision.get(\"timestamp\", datetime.datetime.now().isoformat())\n                    }\n                    readable_results.append(readable_result)\n\n                # Convert to DataFrame\n                df = pd.DataFrame(readable_results)\n\n                # Save to CSV in memory\n                csv_buffer = BytesIO()\n                df.to_csv(csv_buffer, index=False)\n                csv_buffer.seek(0)\n\n                # Save CSV to Supabase\n                results_filename = f\"alert_processing_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n                results_url = self.storage_manager.upload_binary(csv_buffer.getvalue(), results_filename)\n\n\n                logger.info(f\"Human-readable results exported to Supabase: CSV={results_url}\")\n                return {\n                    \"csv_url\": results_url,\n                    \"record_count\": len(readable_results)\n                }\n            else:\n                logger.warning(\"No results to export\")\n                return None\n\n    # Process the alerts\n    async def process_alerts(csv_url):\n        # Initialize the alert processing system\n        aps = AlertProcessingSystem()\n\n        # Load data\n        data = aps.load_csv_data(csv_url)\n\n        # Process each alert\n        results = []\n        for _, row in data.iterrows():\n            alert_id = row['Alert ID']\n            isin = row['ISIN']\n            company_name = row['Company Name']\n\n            logger.info(f\"Processing alert {alert_id} for {company_name} (ISIN: {isin})\")\n\n            # Market type verification\n            market_verification = await aps.verify_market_type(isin, alert_id)\n\n            # Dummy shares verification (skipped as requested)\n            # shares_verification = aps.dummy_verify_outstanding_shares(isin, company_name)\n\n            # Decision making\n            decision = aps.make_final_decision(market_verification)\n\n            # Documentation\n            documentation, report_url = aps.document_alert_processing(alert_id, isin, company_name, market_verification, decision)\n\n            # Add to results\n            result = {\n                \"alert_id\": alert_id,\n                \"isin\": isin,\n                \"company_name\": company_name,\n                \"market_type\": market_verification.get(\"market_type\"),\n                \"is_regulated\": market_verification.get(\"is_regulated\"),\n                \"decision\": decision.get(\"decision\"),\n                \"justification\": decision.get(\"justification\"),\n                \"evidence_url\": market_verification.get(\"evidence_url\"),\n                \"report_url\": report_url\n            }\n            results.append(result)\n\n            # Final processing\n            if decision[\"decision\"] == \"True Positive\":\n                logger.info(f\"Alert {alert_id}: TRUE POSITIVE - Initiating regulatory reporting process\")\n                # This would trigger the regulatory reporting process\n                # Implement actual reporting process code here\n            else:\n                logger.info(f\"Alert {alert_id}: FALSE POSITIVE or INCONCLUSIVE - Closing alert with documentation\")\n\n        # Export results\n        output_info = aps.export_results()\n\n        return {\n            \"completed\": True,\n            \"alerts_processed\": len(results),\n            \"results\": results,\n            \"output_files\": output_info\n        }\n\n    try:\n        processing_results = await process_alerts(csv_url)\n        return f\"\"\"\nAlert processing completed successfully.\n- Processed {processing_results['alerts_processed']} alerts\n- Results exported to:\n  - CSV: {processing_results['output_files']['csv_url']}\n- Detailed evidence and reports saved in Supabase\n\"\"\"\n    except Exception as e:\n        logger.error(f\"Error processing alerts: {e}\", exc_info=True)\n        return f\"Error processing alerts: {str(e)}\"\n","name":"run_alert_processing","description":"\n    Process alerts from the CSV file.\n    \n    Args:\n        csv_url: URL or path to the CSV file\n        \n    Returns:\n        Processing results summary\n    ","global_imports":[],"has_cancellation_support":false}}],"model_context":{"provider":"autogen_core.model_context.UnboundedChatCompletionContext","component_type":"chat_completion_context","version":1,"component_version":1,"description":"An unbounded chat completion context that keeps a view of the all the messages.","label":"UnboundedChatCompletionContext","config":{}},"description":"An agent that provides assistance with ability to use tools.","system_message":"You are an expert financial analyst assistant helping with alert processing.\n        You will help validate market types for securities and document the verification process.\n        You need to analyze alerts, verify market types, create evidence, and make decisions.\n        Process and document each step carefully. All data and evidence will be stored in Supabase.\n        Your workflow is:\n        1. Load the CSV file with alert data containing Alert ID, ISIN (security identifier), and company name\n        2. For each alert:\n           - Verify the market type by checking stock exchange websites (Euronext or Deutsche Börse)\n           - If \"regulated market\" → true positive\n           - If \"growth market\" (France) or similar non-regulated market (Germany) → false positive\n           - Create evidence (screenshots) and save to Supabase\n           - Make a decision (true/false positive)\n           - Document everything in Supabase\n        3. Export the results to Supabase storage","model_client_stream":true,"reflect_on_tool_use":false,"tool_call_summary_format":"{result}","metadata":{}}}